{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 3\n",
    "\n",
    "- In this analysis, I have calculated the frequency of those words that raise suspicion, contained in the emails of Kenneth Lay, one of the CEO's of Enron.\n",
    "- I then calculated the frequency of these words to derive some useful information.\n",
    "- I've used nltk package for the inbuilt methods word_tokenize and FreqDist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\NEU\\Spring 2017\\Data Analysis using Python\\Final assignments\\Midterm\\Question 1\n",
      "D:\\NEU\\Spring 2017\\Data Analysis using Python\\Final assignments\\Midterm\\data\\enron\\maildir\\lay-k\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from email.parser import Parser\n",
    "                     \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(os.getcwd())                        \n",
    "os.chdir('D:\\\\NEU\\\\Spring 2017\\\\Data Analysis using Python\\\\Final assignments\\\\Midterm\\\\data\\\\enron\\\\maildir\\\\lay-k')\n",
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rootdirectory = os.getcwd();\n",
    "\n",
    "# Function to get the email-body of all mails of Kenneth Lay\n",
    "def email_count(inputfile, email_body):\n",
    "    with open(inputfile, \"r\") as outfile:\n",
    "        data = outfile.read()\n",
    "    email = Parser().parsestr(data)\n",
    "    email_body.append(email.get_payload()) #Method to retrieve the body from an email using the Parser\n",
    "    \n",
    "email_body = []      \n",
    "os.chdir('D:\\\\NEU\\\\Spring 2017\\\\Data Analysis using Python\\\\Final assignments\\\\Midterm\\\\Question 1')\n",
    "\n",
    "for directory, subdirectory, filenames in  os.walk(rootdirectory):\n",
    "    for filename in filenames:\n",
    "        email_count(os.path.join(directory, filename), email_body )\n",
    "\n",
    "with open(\"lay_k_body.txt\", \"w\") as file:\n",
    "    for email_bod in email_body:\n",
    "        if email_bod:\n",
    "            file.write(email_bod)\n",
    "            file.write(\"\\n\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of suspicious words in Kenneth Lay's emails:\n",
      "[('bankruptcy', 2297), ('bankrupt', 1116), ('FERC', 43), ('shutdown', 2)]\n"
     ]
    }
   ],
   "source": [
    "words_list=['bankrupt','bankruptcy','shutdown', 'talking points', 'FERC'] \n",
    "with open(\"lay_k_body.txt\", \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "words= word_tokenize(data)\n",
    "count_freq = [word  for word in words if word in words_list]\n",
    "frequency = nltk.FreqDist(count_freq)\n",
    "\n",
    "print(\"Frequency of suspicious words in Kenneth Lay's emails:\")\n",
    "print(frequency.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Thus, we can see that words such as bankruptcy and brankupt were used more than a 1000 times in the mails exchangd with Kenneth Lay.\n",
    "\n",
    "This gives us the information that Kenneth Lay knew they were going to get bankrupt and were avoiding it till the final downfall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Read the file that contains the body of all emails: email_body.txt\n",
    "with open(\"email_body.txt\", \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "'''\n",
    "    We use word_tokenize method to tokenize all the words in the emails. Appending through the words,\n",
    "    we retrieve only those words that are needed.\n",
    "'''\n",
    "    \n",
    "words= word_tokenize(data)\n",
    "real_words = [word  for word in words if word not in stopwords.words('English')]\n",
    "\n",
    "'''\n",
    "FreqDist is a method that lets us calculate the frequency of words in the array and the most_common method lets us retrive \n",
    "the top 100 most frequent words as follows:\n",
    "'''\n",
    "\n",
    "frequency = nltk.FreqDist(real_words)\n",
    "print(frequency.most_common(100))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
